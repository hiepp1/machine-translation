# -*- coding: utf-8 -*-
"""machine_translation_transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iXALveHBoVQiErvz8uAVJcMhOJS7QKMX

DATASET:

**Vietnamese** file: https://drive.google.com/file/d/1F4WFFlA1ceZuGNoqPhXj5cx3fmQbEd15/view?usp=sharing,

**English** file: https://drive.google.com/file/d/1bUy3KTl8S-D1Qc9nYJUBnaRXElevdoMF/view?usp=sharing

**Model**: https://drive.google.com/file/d/1-AV0df0je4t2LcAaGfaDH9dY9Diogh9k/view?usp=drive_link

# Setup for Project
"""

# Install necessary libraries for the project
!pip install pandas torch underthesea matplotlib nltk sacrebleu

# Import required libraries
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import os
import re
import string
import nltk
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction, sentence_bleu
from underthesea import word_tokenize
import math
import random
from timeit import default_timer as timer
import matplotlib.pyplot as plt
import sys
from tqdm import tqdm
import concurrent.futures
import sacrebleu

# Download NLTK data for English tokenization
nltk.download('punkt_tab')

from google.colab import drive
drive.mount('/content/drive')

"""# Data Loading and Preprocessing"""

def load_translation_dataset(en_path, vi_path, max_lines=150000):
    """
    Loads English and Vietnamese sentence pairs from the specified file paths.
    Args:
        en_path (str): Path to the English sentences file.
        vi_path (str): Path to the Vietnamese sentences file.
        max_lines (int): Maximum number of sentence pairs to load.
    Returns:
        pd.DataFrame: A DataFrame containing English and Vietnamese sentence pairs.
        list: Full list of English sentences.
        list: Full list of Vietnamese sentences.
    Raises:
        FileNotFoundError: If the specified files do not exist.
        ValueError: If the number of English and Vietnamese sentences does not match.
    """
    if not os.path.exists(en_path):
        raise FileNotFoundError(f"English sentences file not found at {en_path}")
    if not os.path.exists(vi_path):
        raise FileNotFoundError(f"Vietnamese sentences file not found at {vi_path}")

    # Read sentences from files
    with open(en_path, 'r', encoding='utf-8') as f_en:
        en_sentences = f_en.read().splitlines()
    with open(vi_path, 'r', encoding='utf-8') as f_vi:
        vi_sentences = f_vi.read().splitlines()

    # Validate sentence counts
    if len(en_sentences) != len(vi_sentences):
        raise ValueError(f"Number of English sentences ({len(en_sentences)}) does not match Vietnamese sentences ({len(vi_sentences)})")

    # Create a DataFrame with a subset of the data
    num_lines = min(max_lines, len(en_sentences))
    dataset = pd.DataFrame({
        "english": en_sentences[:num_lines],
        "vietnamese": vi_sentences[:num_lines]
    })

    return dataset, en_sentences, vi_sentences

def clean_text_data(dataframe):
    """
    Preprocesses the text data by removing punctuation, converting to lowercase,
    and normalizing whitespace.
    Args:
        dataframe (pd.DataFrame): DataFrame with 'english' and 'vietnamese' columns.
    Returns:
        pd.DataFrame: Preprocessed DataFrame.
    """
    # Remove punctuation
    remove_punctuation = str.maketrans('', '', string.punctuation)
    dataframe["english"] = dataframe["english"].apply(lambda text: text.translate(remove_punctuation))
    dataframe["vietnamese"] = dataframe["vietnamese"].apply(lambda text: text.translate(remove_punctuation))

    # Convert to lowercase
    dataframe["english"] = dataframe["english"].apply(str.lower)
    dataframe["vietnamese"] = dataframe["vietnamese"].apply(str.lower)

    # Strip whitespace and normalize spaces
    dataframe["english"] = dataframe["english"].apply(lambda text: re.sub(r"\s+", " ", text.strip()))
    dataframe["vietnamese"] = dataframe["vietnamese"].apply(lambda text: re.sub(r"\s+", " ", text.strip()))

    return dataframe

"""# Tokenization and Vocabulary Building"""

def tokenize_english(text):
    """
    Tokenizes an English sentence into individual words using NLTK.
    Args:
        text (str): English sentence to tokenize.
    Returns:
        list: List of tokens.
    """
    return nltk.word_tokenize(text)

def tokenize_vietnamese(text):
    """
    Tokenizes a Vietnamese sentence into individual words using underthesea.
    Args:
        text (str): Vietnamese sentence to tokenize.
    Returns:
        list: List of tokens.
    """
    return word_tokenize(text)

class TranslationVocab:
    """
    A class to manage vocabulary for a language, mapping tokens to indices and vice versa.
    """
    def __init__(self):
        self.token2idx = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}
        self.idx2token = {0: '<unk>', 1: '<pad>', 2: '<bos>', 3: '<eos>'}
        self.current_idx = 4

    def add_word(self, word):
        """
        Adds a new word to the vocabulary if it doesn't exist.
        Args:
            word (str): Word to add.
        """
        if word not in self.token2idx:
            self.token2idx[word] = self.current_idx
            self.idx2token[self.current_idx] = word
            self.current_idx += 1

    def build_from_texts(self, texts, tokenizer_func):
        """
        Builds the vocabulary from a list of texts using the specified tokenizer.
        Args:
            texts (list): List of sentences.
            tokenizer_func (callable): Function to tokenize sentences.
        """
        for text in texts:
            tokens = tokenizer_func(text)
            for token in tokens:
                self.add_word(token)

    def get_index(self, word):
        """
        Returns the index of a word, or the index of '<unk>' if not found.
        Args:
            word (str): Word to look up.
        Returns:
            int: Index of the word.
        """
        return self.token2idx.get(word, self.token2idx['<unk>'])

    def get_word(self, idx):
        """
        Returns the word corresponding to a given index.
        Args:
            idx (int): Index to look up.
        Returns:
            str: Word at the given index.
        """
        return self.idx2token.get(idx, '<unk>')

    def size(self):
        """
        Returns the size of the vocabulary.
        Returns:
            int: Number of unique tokens in the vocabulary.
        """
        return self.current_idx

"""# Model Architecture"""

class PositionalEmbeddings(nn.Module):
    """
    Adds positional embeddings to token embeddings to capture word order in sequences.
    """
    def __init__(self, embed_dim, dropout_rate=0.1, max_length=5000):
        super(PositionalEmbeddings, self).__init__()
        self.dropout = nn.Dropout(dropout_rate)
        pe = torch.zeros(max_length, embed_dim)
        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)

class WordEmbedding(nn.Module):
    """
    Converts token indices into dense embeddings.
    """
    def __init__(self, vocab_size, embed_dim):
        super(WordEmbedding, self).__init__()
        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)
        self.embed_dim = embed_dim

    def forward(self, tokens):
        return self.embedding_layer(tokens.long()) * math.sqrt(self.embed_dim)

class TranslationTransformer(nn.Module):
    """
    Implements a Transformer model for sequence-to-sequence translation.
    """
    def __init__(self, encoder_layers, decoder_layers, embed_dim, num_heads, src_vocab_size, tgt_vocab_size, ff_dim=512, dropout_rate=0.1):
        super(TranslationTransformer, self).__init__()
        self.transformer = nn.Transformer(
            d_model=embed_dim,
            nhead=num_heads,
            num_encoder_layers=encoder_layers,
            num_decoder_layers=decoder_layers,
            dim_feedforward=ff_dim,
            dropout=dropout_rate
        )
        self.output_layer = nn.Linear(embed_dim, tgt_vocab_size)
        self.src_embedding = WordEmbedding(src_vocab_size, embed_dim)
        self.tgt_embedding = WordEmbedding(tgt_vocab_size, embed_dim)
        self.positional_embed = PositionalEmbeddings(embed_dim, dropout_rate)

    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_padding_mask):
        src_embedded = self.positional_embed(self.src_embedding(src))
        tgt_embedded = self.positional_embed(self.tgt_embedding(tgt))
        output = self.transformer(
            src_embedded, tgt_embedded, src_mask, tgt_mask, None,
            src_padding_mask, tgt_padding_mask, memory_padding_mask
        )
        return self.output_layer(output)

    def encode_sequence(self, src, src_mask):
        """
        Encodes the source sequence using the Transformer encoder.
        """
        return self.transformer.encoder(self.positional_embed(self.src_embedding(src)), src_mask)

    def decode_sequence(self, tgt, memory, tgt_mask):
        """
        Decodes the target sequence using the Transformer decoder.
        """
        return self.transformer.decoder(self.positional_embed(self.tgt_embedding(tgt)), memory, tgt_mask)

"""# Data Preparation for Training"""

def sentence_to_tensor(text, vocab, tokenizer_func):
    """
    Converts a sentence into a tensor of indices with BOS and EOS tokens.
    Args:
        text (str): Sentence to convert.
        vocab (TranslationVocab): Vocabulary object.
        tokenizer_func (callable): Function to tokenize the sentence.
    Returns:
        torch.Tensor: Tensor of indices.
    """
    tokens = tokenizer_func(text)
    indices = [2] + [vocab.get_index(token) for token in tokens] + [3]  # <bos> and <eos>
    return torch.tensor(indices, dtype=torch.long)

def batch_collate_fn(batch, src_vocab, tgt_vocab):
    """
    Collates a batch of sentence pairs into padded tensors for training.
    Args:
        batch (list): List of (source, target) sentence pairs.
        src_vocab (TranslationVocab): Source language vocabulary.
        tgt_vocab (TranslationVocab): Target language vocabulary.
    Returns:
        tuple: Padded source and target tensors.
    """
    src_tensors, tgt_tensors = [], []
    for src_text, tgt_text in batch:
        src_tensors.append(sentence_to_tensor(src_text.rstrip("\n"), src_vocab, tokenize_english))
        tgt_tensors.append(sentence_to_tensor(tgt_text.rstrip("\n"), tgt_vocab, tokenize_vietnamese))
    src_padded = torch.nn.utils.rnn.pad_sequence(src_tensors, padding_value=1)  # <pad>
    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_tensors, padding_value=1)  # <pad>
    return src_padded, tgt_padded

def create_attention_masks(src, tgt, device):
    """
    Creates attention masks for the Transformer model.
    Args:
        src (torch.Tensor): Source tensor.
        tgt (torch.Tensor): Target tensor.
        device (torch.device): Device to place the masks on.
    Returns:
        tuple: Source mask, target mask, source padding mask, target padding mask.
    """
    src_len = src.shape[0]
    tgt_len = tgt.shape[0]
    tgt_mask = (torch.triu(torch.ones((tgt_len, tgt_len), device=device)) == 1).transpose(0, 1)
    tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))
    src_mask = torch.zeros((src_len, src_len), device=device).type(torch.bool)
    src_padding_mask = (src == 1).transpose(0, 1)  # <pad>
    tgt_padding_mask = (tgt == 1).transpose(0, 1)  # <pad>
    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask

"""# Training and Evaluation"""

class TrainingStopper:
    """
    Implements early stopping to halt training if validation loss stops improving.
    """
    def __init__(self, patience=5, min_improvement=0.1):
        self.patience = patience
        self.min_improvement = min_improvement
        self.counter = 0
        self.should_stop = False

    def check_stopping(self, train_loss, val_loss):
        """
        Checks if training should stop based on loss difference.
        Args:
            train_loss (float): Training loss.
            val_loss (float): Validation loss.
        """
        if (val_loss - train_loss) > self.min_improvement:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True

def train_one_epoch(model, optimizer, dataloader, loss_fn, device):
    """
    Trains the model for one epoch.
    Args:
        model (nn.Module): The Transformer model.
        optimizer (torch.optim.Optimizer): Optimizer for training.
        dataloader (DataLoader): Training data loader.
        loss_fn (nn.Module): Loss function.
        device (torch.device): Device to train on.
    Returns:
        float: Average training loss for the epoch.
    """
    model.train()
    epoch_loss = 0
    for src_batch, tgt_batch in dataloader:
        src_batch = src_batch.to(device)
        tgt_batch = tgt_batch.to(device)
        tgt_input = tgt_batch[:-1, :]
        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_attention_masks(src_batch, tgt_input, device)
        logits = model(src_batch, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, src_pad_mask)
        optimizer.zero_grad()
        tgt_output = tgt_batch[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_output.reshape(-1))
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    return epoch_loss / len(dataloader)

def evaluate_model(model, dataloader, loss_fn, device):
    """
    Evaluates the model on the validation set.
    Args:
        model (nn.Module): The Transformer model.
        dataloader (DataLoader): Validation data loader.
        loss_fn (nn.Module): Loss function.
        device (torch.device): Device to evaluate on.
    Returns:
        float: Average validation loss.
    """
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for src_batch, tgt_batch in dataloader:
            src_batch = src_batch.to(device)
            tgt_batch = tgt_batch.to(device)
            tgt_input = tgt_batch[:-1, :]
            src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_attention_masks(src_batch, tgt_input, device)
            logits = model(src_batch, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, src_pad_mask)
            tgt_output = tgt_batch[1:, :]
            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_output.reshape(-1))
            epoch_loss += loss.item()
    return epoch_loss / len(dataloader)

def decode_greedy(model, src, src_mask, max_len, start_symbol, device):
    """
    Decodes a sequence using greedy decoding.
    Args:
        model (nn.Module): The Transformer model.
        src (torch.Tensor): Source tensor.
        src_mask (torch.Tensor): Source mask.
        max_len (int): Maximum length of the decoded sequence.
        start_symbol (int): Starting symbol index (<bos>).
        device (torch.device): Device to perform decoding on.
    Returns:
        torch.Tensor: Decoded sequence.
    """
    src = src.to(device)
    src_mask = src_mask.to(device)
    memory = model.encode_sequence(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)
    for _ in range(max_len - 1):
        tgt_mask = (torch.triu(torch.ones((ys.size(0), ys.size(0)), device=device)) == 1).transpose(0, 1)
        tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 0, float('-inf')).masked_fill(tgt_mask == 1, float(0.0))
        out = model.decode_sequence(ys, memory, tgt_mask)
        out = out.transpose(0, 1)
        prob = model.output_layer(out[:, -1])
        _, next_token = torch.max(prob, dim=1)
        next_token = next_token.item()
        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_token)], dim=0)
        if next_token == 3:  # <eos>
            break
    return ys

def translate_sentence(model, sentence, src_vocab, tgt_vocab, device):
    """
    Translates a single English sentence to Vietnamese.
    Args:
        model (nn.Module): The Transformer model.
        sentence (str): English sentence to translate.
        src_vocab (TranslationVocab): Source vocabulary.
        tgt_vocab (TranslationVocab): Target vocabulary.
        device (torch.device): Device to perform translation on.
    Returns:
        str: Translated Vietnamese sentence.
    """
    model.eval()
    src = sentence_to_tensor(sentence, src_vocab, tokenize_english).view(-1, 1)
    num_tokens = src.shape[0]
    src_mask = torch.zeros(num_tokens, num_tokens).type(torch.bool)
    tgt_indices = decode_greedy(model, src, src_mask, max_len=num_tokens + 5, start_symbol=2, device=device).flatten()
    translated = " ".join(tgt_vocab.get_word(idx) for idx in tgt_indices.cpu().numpy() if idx not in [2, 3]).strip()
    return translated

def calculate_bleu_sacrebleu(predictions, references):
    return sacrebleu.corpus_bleu(predictions, [references]).score

def generate_parallel_translations(model, src_vocab, tgt_vocab, test_data, device):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        predictions = list(executor.map(lambda sentence: translate_sentence(model, sentence, src_vocab, tgt_vocab, device), test_data))
    return predictions

"""# Main"""

ENGLISH_PATH = "/content/drive/MyDrive/Dataset/en_sents"
VIETNAMESE_PATH = "/content/drive/MyDrive/Dataset/vi_sents"
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
BATCH_SIZE = 64
NUM_EPOCHS = 20

# Load and preprocess the dataset
dataset, all_english, all_vietnamese = load_translation_dataset(ENGLISH_PATH, VIETNAMESE_PATH)

print(dataset.head(10))

dataset = clean_text_data(dataset)

print(dataset.head(10))

src_vocab = TranslationVocab()
tgt_vocab = TranslationVocab()

src_vocab.build_from_texts(dataset['english'], tokenize_english)
tgt_vocab.build_from_texts(dataset['vietnamese'], tokenize_vietnamese)

print(f"\nSource (English) vocabulary size: {src_vocab.size()}")
print(f"Target (Vietnamese) vocabulary size: {tgt_vocab.size()}")

sample_en_text = dataset["english"].iloc[0]
sample_vi_text = dataset["vietnamese"].iloc[0]

print(f"\nSample English sentence: {sample_en_text}")
print(f"Tokenized English: {tokenize_english(sample_en_text)}")
print(f"Sample Vietnamese sentence: {sample_vi_text}")
print(f"Tokenized Vietnamese: {tokenize_vietnamese(sample_vi_text)}")

split_ratio = 0.9
split_point = int(len(dataset) * split_ratio)
train_set = list(zip(dataset['english'][:split_point], dataset['vietnamese'][:split_point]))
val_set = list(zip(dataset['english'][split_point:], dataset['vietnamese'][split_point:]))

print(f"\nTraining set size: {len(train_set)}")
print(f"Validation set size: {len(val_set)}")

"""## Train model"""

train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=lambda batch: batch_collate_fn(batch, src_vocab, tgt_vocab))
val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=lambda batch: batch_collate_fn(batch, src_vocab, tgt_vocab))

# Initialize the model
model = TranslationTransformer(
    encoder_layers=4,
    decoder_layers=4,
    embed_dim=512,
    num_heads=8,
    src_vocab_size=src_vocab.size(),
    tgt_vocab_size=tgt_vocab.size(),
    ff_dim=512,
    dropout_rate=0.1
)

# Initialize model weights
for param in model.parameters():
    if param.dim() > 1:
        nn.init.xavier_uniform_(param)

model = model.to(DEVICE)

# Define loss function and optimizer
loss_function = nn.CrossEntropyLoss(ignore_index=1)  # <pad>
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

stopper = TrainingStopper(patience=5, min_improvement=0.1)
training_history = {"train_loss": [], "val_loss": []}

for epoch in range(1, NUM_EPOCHS + 1):
    start_time = timer()
    train_loss = train_one_epoch(model, optimizer, train_loader, loss_function, DEVICE)
    val_loss = evaluate_model(model, val_loader, loss_function, DEVICE)
    end_time = timer()
    training_history["train_loss"].append(train_loss)
    training_history["val_loss"].append(val_loss)
    print(f"Epoch {epoch}/{NUM_EPOCHS} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | Time: {(end_time - start_time):.3f}s")
    stopper.check_stopping(train_loss, val_loss)
    if stopper.should_stop:
        print(f"Early stopping at epoch {epoch}")
        break

plt.figure(figsize=(10, 5))
plt.plot(training_history["train_loss"], label="Training Loss", color="blue")
plt.plot(training_history["val_loss"], label="Validation Loss", color="orange")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.grid(True)
plt.show()

test_indices = range(len(all_english) // 2)
test_pairs = [(all_english[i], all_vietnamese[i]) for i in test_indices]

bleu_scores = []
predictions = []

references = [pair[1] for pair in test_pairs]

for batch_start in tqdm(range(0, len(test_pairs), 64), desc="Processing batches", unit="batch"):
    batch_end = min(batch_start + 64, len(test_pairs))
    batch_src = [pair[0] for pair in test_pairs[batch_start:batch_end]]

    batch_predictions = generate_parallel_translations(model, src_vocab, tgt_vocab, batch_src, DEVICE)
    predictions.extend(batch_predictions)

bleu_score = calculate_bleu_sacrebleu(predictions, references)

print(f"BLEU Score (sacrebleu): {bleu_score:.4f}")

"""## Test translation"""

test_indices = random.sample(range(6000, len(all_english)), 5)
test_pairs = [(all_english[i], all_vietnamese[i]) for i in test_indices]

print("\nTranslation Results on Test Samples:\n")
for idx, (en, vi) in enumerate(test_pairs, 1):
    en_cleaned = clean_text_data(pd.DataFrame({"english": [en], "vietnamese": [""]}))["english"].iloc[0]
    predicted_vi = translate_sentence(model, en_cleaned, src_vocab, tgt_vocab, DEVICE)
    print(f"Test Sample {idx}:")
    print(f"English Input: {en}")
    print(f"Actual Vietnamese: {vi}")
    print(f"Predicted Vietnamese: {predicted_vi}")
    print("-" * 60)

# Test on a custom sentence
custom_sentence = "I dont think she likes you!"
custom_cleaned = clean_text_data(pd.DataFrame({"english": [custom_sentence], "vietnamese": [""]}))["english"].iloc[0]
custom_predicted = translate_sentence(model, custom_cleaned, src_vocab, tgt_vocab, DEVICE)

print("\nCustom Sentence Translation:")
print(f"English Input: {custom_sentence}")
print(f"Predicted Vietnamese: {custom_predicted}")

"""## Save and Load model (if necessary)"""

# --------------- SAVE THE MODEL ----------------- #
# model_save_path = "/content/drive/MyDrive/model/transformer_model.pth"
# torch.save(model.state_dict(), model_save_path)
# print(f"\nModel saved to {model_save_path}")

# --------------- LOAD THE MODEL ----------------- #
# model.load_state_dict(torch.load("/content/drive/MyDrive/model/transformer_model.pth"))
# model.to(DEVICE)
# model.eval()